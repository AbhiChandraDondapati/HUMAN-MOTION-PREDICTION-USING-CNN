{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTAMJy5vrvys"
      },
      "outputs": [],
      "source": [
        "# Code for Live Camera Feed\n",
        "# !pip install keras tensorflow opencv-python numpy pandas scikit-learn matplotlib\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# Pre process data\n",
        "\n",
        "def live_camera_input():\n",
        "    cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
        "\n",
        "    img_rows, img_cols, img_depth = 16, 16, 15\n",
        "    frames = []\n",
        "\n",
        "    label_map = {0: 'Boxing', 1: 'HandWaving', 2: 'Jogging', 3: 'Running', 4: 'Walking'}\n",
        "\n",
        "    print(\"Press 'q' to exit.\")\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, (img_rows, img_cols))\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        frames.append(gray)\n",
        "\n",
        "        if len(frames) == img_depth:\n",
        "            input_video = np.array(frames).astype('float32') / 255\n",
        "            input_video = np.rollaxis(np.rollaxis(input_video, 2, 0), 2, 0)\n",
        "            input_video = input_video.reshape(1, 1, img_rows, img_cols, img_depth)\n",
        "\n",
        "            predicted_class = model.predict(input_video)\n",
        "            predicted_label = np.argmax(predicted_class)\n",
        "            print(\"Predicted Action:\", label_map[predicted_label])\n",
        "\n",
        "            # Clear frames for the next prediction\n",
        "            frames = []\n",
        "\n",
        "        cv2.imshow('Live Camera', frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Run live camera input\n",
        "live_camera_input()\n",
        "\n"
      ]
    }
  ]
}